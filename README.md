# Assignment 4 - Image classification of scraped dataset from Pexels
This is the fourth and final assignment in the Visual Analytics course. It is also the self assigned assignment

# Contribution
This assignment was made on my own with now contribution from my fellow students. The assignment delves into image scraping using a personal API key from Pexels. Since this is new to me, i consulted a [StackOverflow](https://stackoverflow.com/questions/63852440/how-do-i-download-videos-from-pexels-api) thread for inspiration on how to scrape the desired amount of images i wanted. For splitting the scraped images into training and testing data and making their respective folders into labels i consulted the following [tutorial](https://medium.com/mlearning-ai/image-classification-for-beginner-a6de7a69bc78). In particular in building a query to label dictionary.

# Data
The data for this assignment is scraped directly from Pexels using my personal API key that i applied for. The data i acquired through a script in the _src_ folder called _scrape.py_. The script is set up so that based on what query is run from the command line, a maximun of 240 images are downloaded to a label folder in the _in_ folder. It means that the _in_ folder is empty before the code is run, but a dictionary called _query_to_folder_ shows the predefined queries i used to get the exact data i trained the classifiers on. I originally planned to use scraping pipeline for Google Images, but since most of the downloaded images were copyright protected, i resorted to free to use non-copyrighted images from Pexels. Although this did result in some big inconsistencies in the quality of the data from the different queries because there simply isnt a comparable amount of images on Pexels to Google Images.

# Packages
I used a variety of different packages accross the scripts in this repository. I will list them and explain for what purpose they were included
* ```os``` is used to navigate paths.
* ```Json``` is used in downloading images in the _scrape.py_ script
* ```argparse``` is used to make it possible for the user to specify which query they want to download images for
* ```requests``` is used for requesting to download images from Pexels using my API key
* ```Numpy``` is used to make arrays
* ```OpenCV``` is used to grayscaling and resizing the images 
* ```scikit-learn``` is used to train the data on a logistic regression classifier and a neural network classifier

# Method
This self chosen assignment consists of three different scripts. The first script i will run through is the _scrape.py_ script. Firstly the ```argparse``` command line argument is set up, so that the user can specify which query they want to download images for. then the script sets up search parameters. In this case it scrapes 80 images per page and the maximum amount of images is set to 240. Then my API key is identified and i create a dictionary _query_to_folder_ which allows the user to see which predefined queries i used to get images for classifier training. I have created an option that creates an _other_ folder if one chooses to use a different query than one of those i used. Then the script runs a for loop that iterates through each image on each page and requests the URL's. After the URL's have been gathered for the desired images, the _get_ argument from ```requests``` is used to download the images frin their URL's. Then the script starts downloading each iamge from the top, printing a message for the user to keep track of proceedings until it reaches 240 downloaded images.

The second and third scripts of this assignment are based on the pipelines for the scripts in assignment 2. They both train classifiers on the rather small sized dataset of scraped images sorted into 5 labels. Firstly, a list of the class labels is created for later appending. The approach in the preprocessing steps are a bit different in the pipelines of _logreg.py_ and _nnclass.py_ compared to those of the scripts from assignment 2. Here i create a for loop that iterates through every iamge, grayscales it and resizes it using ```cv2.imread``` and the argument _cv2.IMREAD_GRAYSCALE_ as well as ```cv2.resize```. Then it appends the preprocessed images. Then the scripts convert the images to ```Numpy``` arrays and reshapes them before it is split into training and test splits using ```train_test_split()``` from ```OpenCV```. Now the classification models are built. The logistic regression classifier is set with a tolerance of 0.1 in order to minimize waste in computational resources spent if the classifier doesnt perform. Then predictions are made based on the classifier's training and a classification report follows up before it is printed to the out folder under the name "logistic_reg_classifier_metrics.txt". For the neural network classifier the methods used are the same. It is the same preprocessing steps that are made. It is only the model training itself that is different. Here a neural network classifier is trained on the Cifar10 train and test split. Of the parameters for the nn classifier, the adaptive learning rate and early_stopping=True are most important to grasp if one is interested in preventing the model training going nowhere. The comments provided in the pipeline goes furhter in depth with the different parameters chosen during training. The output of this script is called "neural_network_classifier_metrics.txt" and is also located in the out folder.

# Discussion of results
The output of the two classifiers are vastly different. The logistic regression classifier outperforms the neural network classifier substantially. Based on the very small amount of training data it manages to acquire a f1-score of 0.34. Where the highest accuracy score is on the _tennis_ label. The neural network classifier, on the other hand, performs considerably worse. Only amounting a f1-score of 0.21. But the interesting thing is that it only made predictions on one label which is _table_tennis_. This might indicate that the complexity of the neural network classifier compared to the logistic regression classifier in this case is not beneficial due to the very limited amount of training data. It is clear that the more linear logistic regression model is more capable at working with small datasets. However, if one wishes to upscale the maximum amount of scraped images, they can do so in the _total_images_ object in the _scrape.py_ script. However, it did take a couple of minutes for each query to download 240 images, so i scaled it down for the sake of time consumption. But the pipeline works just as well with an upscaled dataset.

# Usage
!OBS! it is important that the scripts are run in the order i give below because the _scrape.py_ creates the datasets that the _logreg.py_ and _nnclass.py_ scripts train on. 
* To run this code you run the setup script by running ```bash setup.sh```. This installs the requirements and creates the virtual environment
* Then run ```python3 -m venv assignment4_env``` from the command line. This activates the virtual environment
* Now for creating the data i recommend running the _scrape.py_ script from the command line with the 5 predefines queries: "badminton play", "padel tennis", "squash sport", "table tennis", "tennis sport". Here is an example of how to run the code with "badminton play" as a query made with ```argparse```
* Type ```python3 src/scrape.py --query "badminton play"
* This will create a folder called _badminton_ in the _in_ folder where the images will be stored. Do the same for the rest of the queries and the dataset will be complete as i used it for the classifiers.
* Then run ```python3 src/logreg.py``` from the command line
* And run ```python3 src/nnclass.py``` from the command line
* This will give you the classification reports that are stored in the _out_ folder
